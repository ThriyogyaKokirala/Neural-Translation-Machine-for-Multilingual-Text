{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc23bae",
   "metadata": {},
   "source": [
    "# Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8027c9a0",
   "metadata": {},
   "source": [
    "# Professor: Khaled Sayed\n",
    "                       Presenter: 1. Thriyogya Kokirala            ID:00785999\n",
    "                                  2. Poojitha Mandapati           ID:00797556\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8316cb8",
   "metadata": {},
   "source": [
    "# Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8828394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_file = 'language processing dataset/Language Detection.csv'\n",
    "df = pd.read_csv(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "190d7c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = df['Text'].tolist()\n",
    "target_languages = df['Language'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db4ceb6",
   "metadata": {},
   "source": [
    "# Step 2: Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a099202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join(char for char in text if char.isalpha() or char.isspace())\n",
    "    return text\n",
    "\n",
    "preprocessed_texts = [preprocess_text(text) for text in input_texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30667536",
   "metadata": {},
   "source": [
    "# Step 3: Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b76ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\new anaconda\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\new anaconda\\lib\\site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b07ef0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Text: ' nature in the broadest sense is the natural physical material world or universe' | Detected Language: en\n",
      "Preprocessed Text: 'nature can refer to the phenomena of the physical world and also to life in general' | Detected Language: en\n",
      "Preprocessed Text: 'the study of nature is a large if not the only part of science' | Detected Language: en\n",
      "Preprocessed Text: 'although humans are part of nature human activity is often understood as a separate category from other natural phenomena' | Detected Language: en\n",
      "Preprocessed Text: ' the word nature is borrowed from the old french nature and is derived from the latin word natura or essential qualities innate disposition and in ancient times literally meant birth' | Detected Language: en\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect_langs\n",
    "\n",
    "# Function to detect language with confidence threshold\n",
    "def detect_language_with_threshold(input_text, confidence_threshold=0.8):\n",
    "    try:\n",
    "        lang_results = detect_langs(input_text)\n",
    "        most_probable_lang = lang_results[0]\n",
    "        \n",
    "        if most_probable_lang.lang == 'unknown' or most_probable_lang.prob < confidence_threshold:\n",
    "            return \"Unknown\"\n",
    "        else:\n",
    "            return most_probable_lang.lang\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting language for text: {input_text}\")\n",
    "        return \"Unknown\"\n",
    "    \n",
    "for text in preprocessed_texts[:5]:\n",
    "    detected_language = detect_language_with_threshold(text)\n",
    "    print(f\"Preprocessed Text: '{text}' | Detected Language: {detected_language}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba98ca",
   "metadata": {},
   "source": [
    "# Step 4: Load Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1fb0f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\new anaconda\\lib\\site-packages (0.1.99)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae75aa6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5782da21fa34241a2eb8390973358aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/265 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\New anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Janet\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b5784c5fcc4cadb7fd10613670a8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\New anaconda\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bebe16fe3dc4fde9b465bdefad904a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005cdb3881c4473bbca66858c9bd754b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-ROMANCE'  # English to Romance languages\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67f2cf",
   "metadata": {},
   "source": [
    "# Step 5: Text Translation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f420d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text (fr): ' Nature, in the broadest sense, is the natural, physical, material world or universe.' | Translated Text: La naturaleza, au sens large, est el mundo natural, físico, material, o universo.\n",
      "Input Text (fr): '\"Nature\" can refer to the phenomena of the physical world, and also to life in general.' | Translated Text: \"Naturaleza\" puede referirse a los fenómenos del mundo físico, y también a la vida en general.\n",
      "Input Text (fr): 'The study of nature is a large, if not the only, part of science.' | Translated Text: El estudio de la naturaleza es una parte importante, se no la única, de la ciencia.\n",
      "Input Text (fr): 'Although humans are part of nature, human activity is often understood as a separate category from other natural phenomena.' | Translated Text: Bien que les seres humanos formen parte de la naturaleza, la actividad humana se entend souvent como una categoría separada de otros fenómenos naturales.\n",
      "Input Text (fr): '[1] The word nature is borrowed from the Old French nature and is derived from the Latin word natura, or \"essential qualities, innate disposition\", and in ancient times, literally meant \"birth\".' | Translated Text: [1] O termo natureza é tomado emprestado da Natureza Old French e deriva do termo latino natura, ou \"qualidades esenciales, disposição innata\", e, em tempos antigos, literalmente significa \"nascimento\".\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to translate text\n",
    "def translate_text(text, target_language='fr'):  # Specify the target language code\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "    # Generate translation\n",
    "    translation_ids = model.generate(input_ids, max_length=150, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode and return the translation\n",
    "    translated_text = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Example usage for translation\n",
    "target_language_code = 'fr'  # Example: French\n",
    "for i, text in enumerate(input_texts[:5]):\n",
    "    translation = translate_text(text, target_language=target_language_code)\n",
    "    print(f\"Input Text ({target_language_code}): '{text}' | Translated Text: {translation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e86e73",
   "metadata": {},
   "source": [
    "# Step 6: Calculate BLEU Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "591990e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 5.379525625492818\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "reference_translations = [['reference_translation_1', 'reference_translation_2'], ['reference_translation_3', 'reference_translation_4']]\n",
    "\n",
    "# Placeholder for `input_texts`\n",
    "input_texts = ['input_text_1', 'input_text_2', 'input_text_3', 'input_text_4']\n",
    "\n",
    "# the target language code\n",
    "target_language_code = 'fr'\n",
    "\n",
    "def translate_text(text, target_language):\n",
    "    translated_text = f\"Translated: {text}\" \n",
    "    return translated_text\n",
    "\n",
    "hypotheses = [translate_text(text, target_language=target_language_code) for text in input_texts]\n",
    "\n",
    "references_combined = [\" \".join(refs) for refs in reference_translations]\n",
    "\n",
    "# Calculate BLEU score using sacrebleu\n",
    "bleu_score = sacrebleu.corpus_bleu(hypotheses, [references_combined])\n",
    "print(f\"BLEU Score: {bleu_score.score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2f942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
